# üß† ADAPTIVE LEARNING & DYNAMIC MOCK EXAMS - LOGIC EXPLANATION

## üìä PART 1: ADAPTIVE LEARNING LOGIC

### **The Problem:**
Users get frustrated if questions are too hard, or bored if too easy. We need to match question difficulty to their skill level.

### **The Solution:**
Track performance ‚Üí Calculate difficulty level ‚Üí Select matching questions ‚Üí Adjust in real-time

---

## üîÑ ADAPTIVE LEARNING FLOW

### **Step 1: Calculate User's Difficulty Level**

```
INPUT: User's test history
OUTPUT: Difficulty level (1-10)

LOGIC:
1. Look at last 5 practice tests
2. Calculate average score
3. Calculate consistency (how similar scores are)
4. Calculate improvement trend (getting better or worse)

FORMULA:
- Average Score 90%+ ‚Üí Difficulty 8-10 (Expert)
- Average Score 70-89% ‚Üí Difficulty 5-7 (Intermediate)
- Average Score 50-69% ‚Üí Difficulty 3-5 (Beginner-Intermediate)
- Average Score <50% ‚Üí Difficulty 1-3 (Beginner)

ADJUSTMENTS:
- Consistent scores ‚Üí +1 difficulty (ready for harder)
- Improving trend ‚Üí +1 difficulty (getting better)
- Declining trend ‚Üí -1 difficulty (need easier)
- High variance ‚Üí -1 difficulty (unstable, go easier)
```

**Example:**
```
User's last 5 tests: [65%, 70%, 75%, 80%, 78%]
- Average: 73.6%
- Consistency: Good (all 65-80%)
- Trend: Improving (65% ‚Üí 78%)

Calculation:
- Base difficulty: 5 (from 73.6% average)
- +1 for consistency
- +1 for improvement
- Final: Difficulty 7 (Advanced)
```

---

### **Step 2: Select Questions Based on Difficulty**

```
INPUT: Difficulty level (1-10), Test topic
OUTPUT: Questions matching difficulty

LOGIC:
1. Get all questions for the topic
2. Filter by difficulty tag
3. Select questions matching user's level

DIFFICULTY MAPPING:
- Level 1-3: Only "easy" questions
- Level 4-6: Mix of "easy" (40%) + "medium" (60%)
- Level 7-8: Mix of "medium" (50%) + "hard" (50%)
- Level 9-10: Only "hard" questions
```

**Example:**
```
User difficulty: 7 (Advanced)
Topic: Priority Rules

Available questions:
- Easy: 20 questions
- Medium: 30 questions
- Hard: 15 questions

Selection:
- 10 medium questions (50%)
- 10 hard questions (50%)
- Total: 20 questions
```

---

### **Step 3: Adjust Difficulty After Each Answer**

```
INPUT: Current difficulty, Answer correct?, Time spent
OUTPUT: New difficulty level

LOGIC:
After each question:
- Correct + Fast (<30s) ‚Üí +0.5 difficulty (getting better)
- Correct + Slow (>60s) ‚Üí +0.2 difficulty (correct but unsure)
- Wrong + Fast (<30s) ‚Üí -0.5 difficulty (rushing, need easier)
- Wrong + Slow (>60s) ‚Üí -0.3 difficulty (struggling, need easier)

RULES:
- Don't adjust more than ¬±1 per test
- Don't go below 1 or above 10
- Next question uses new difficulty
```

**Example:**
```
Current difficulty: 6
User answers question:
- Correct in 25 seconds (fast)
- Adjustment: +0.5
- New difficulty: 6.5 ‚Üí rounds to 7

Next question: Select from difficulty 7 pool
```

---

## üìù PART 2: DYNAMIC MOCK EXAM LOGIC

### **The Problem:**
Generic mock exams don't help users improve weak areas. We need personalized exams that focus on what they need to practice.

### **The Solution:**
Analyze performance ‚Üí Identify weak areas ‚Üí Filter real CBR questions ‚Üí Generate personalized exam

---

## üîÑ DYNAMIC MOCK EXAM FLOW

### **Step 1: Analyze User Performance**

```
INPUT: All test history
OUTPUT: Weak areas list

LOGIC:
1. Group results by topic/subject
2. Calculate average score per topic
3. Identify topics with <60% average
4. Sort by worst performance first

TOPIC MAPPING:
Practice Test ‚Üí Real Exam Subject
- "priority-rules" ‚Üí "Priority Rules"
- "hazard-perception" ‚Üí "Hazard Perception"
- "speed-safety" ‚Üí "Speed Limits"
- etc.
```

**Example:**
```
User's test history:
- Priority Rules: 45% (took 3 times)
- Hazard Perception: 55% (took 2 times)
- Speed Limits: 75% (took 1 time)
- Traffic Lights: 85% (took 2 times)

Weak Areas Analysis:
1. Priority Rules: 45% (worst)
2. Hazard Perception: 55% (needs work)
3. Speed Limits: 75% (okay, not weak)
4. Traffic Lights: 85% (strong)

Weak Areas: ["Priority Rules", "Hazard Perception"]
```

---

### **Step 2: Filter Real CBR Questions by Weak Areas**

```
INPUT: Weak areas, All real exam questions
OUTPUT: Filtered questions from weak areas

LOGIC:
1. Get ALL real exam questions (realExamQuestions + mockExamImageQuestions)
2. Filter where subject matches weak areas
3. Separate into regular (no image) and image questions
4. Maintain CBR format: 15 regular + 10 image

IMPORTANT:
- ONLY filter, NEVER generate
- ONLY use real CBR questions
- Preserve all question properties
```

**Example:**
```
Weak Areas: ["Priority Rules", "Hazard Perception"]

All Real Questions: 200 questions total
- Priority Rules: 30 questions (20 regular, 10 image)
- Hazard Perception: 25 questions (15 regular, 10 image)
- Other topics: 145 questions

Filtered Questions:
- Priority Rules: 30 questions
- Hazard Perception: 25 questions
- Total from weak areas: 55 questions
```

---

### **Step 3: Select Questions for Exam**

```
INPUT: Filtered questions, Target: 25 questions
OUTPUT: Personalized exam (15 regular + 10 image)

LOGIC:
1. Prioritize weak area questions
2. Select 15 regular questions (no image)
3. Select 10 image questions
4. If not enough, fill from other real questions
5. Shuffle to randomize order

DISTRIBUTION:
- 60% from weak areas (15 questions)
- 30% from medium areas (7 questions)
- 10% from strong areas (3 questions)
```

**Example:**
```
Filtered Questions:
- Priority Rules: 20 regular, 10 image
- Hazard Perception: 15 regular, 10 image

Selection:
1. Select 10 regular from Priority Rules (weakest)
2. Select 5 regular from Hazard Perception
3. Select 6 image from Priority Rules
4. Select 4 image from Hazard Perception
5. Total: 15 regular + 10 image = 25 questions

If not enough:
- Fill remaining from other real questions
- Still maintain 15 regular + 10 image format
```

---

### **Step 4: Balance Difficulty**

```
INPUT: Selected questions, User's difficulty level
OUTPUT: Balanced exam with appropriate difficulty

LOGIC:
1. Get user's current difficulty level (from adaptive learning)
2. Filter questions by difficulty tag
3. Distribute: Easy (20%), Medium (50%), Hard (30%)
4. Ensure mix matches user's level

DIFFICULTY DISTRIBUTION:
- Beginner (1-3): Easy 50%, Medium 50%
- Intermediate (4-6): Easy 20%, Medium 60%, Hard 20%
- Advanced (7-8): Easy 10%, Medium 50%, Hard 40%
- Expert (9-10): Easy 0%, Medium 30%, Hard 70%
```

**Example:**
```
User difficulty: 6 (Intermediate)
Selected 25 questions from weak areas

Distribution:
- Easy: 5 questions (20%)
- Medium: 12 questions (50%)
- Hard: 8 questions (30%)

Final Exam: 25 questions, balanced difficulty
```

---

## üéØ COMPLETE EXAMPLE FLOW

### **Scenario: User wants to take Mock Exam 1**

#### **Step 1: Analyze Performance**
```
Test History:
- Priority Rules: 3 attempts, avg 45%
- Hazard Perception: 2 attempts, avg 55%
- Speed Limits: 1 attempt, 75%
- Traffic Lights: 2 attempts, 85%

Weak Areas Identified:
1. Priority Rules (45%)
2. Hazard Perception (55%)
```

#### **Step 2: Get Real CBR Questions**
```
All Real Questions Database:
- realExamQuestions: 150 questions
- mockExamImageQuestions: 50 questions
- Total: 200 real CBR questions
```

#### **Step 3: Filter by Weak Areas**
```
Filter: subject = "Priority Rules" OR "Hazard Perception"

Results:
- Priority Rules: 30 questions (20 regular, 10 image)
- Hazard Perception: 25 questions (15 regular, 10 image)
- Total: 55 questions from weak areas
```

#### **Step 4: Select Exam Questions**
```
Target: 25 questions (15 regular + 10 image)

From Weak Areas:
- 10 regular from Priority Rules
- 5 regular from Hazard Perception
- 6 image from Priority Rules
- 4 image from Hazard Perception
= 25 questions total

All questions are REAL CBR questions!
```

#### **Step 5: Balance Difficulty**
```
User difficulty: 6 (Intermediate)

Distribution:
- Easy: 5 questions
- Medium: 12 questions
- Hard: 8 questions
```

#### **Step 6: Generate Exam**
```
Final Exam:
- 25 REAL CBR questions
- 15 regular questions (no image)
- 10 image questions
- Focused on: Priority Rules, Hazard Perception
- Difficulty: Balanced for intermediate level
- Format: Matches real CBR exam exactly
```

---

## üîë KEY PRINCIPLES

### **1. Adaptive Learning:**
- **Track everything:** Every answer, time spent, topic
- **Calculate dynamically:** Difficulty adjusts in real-time
- **Match user level:** Questions match current ability
- **Progressive:** Gradually increases difficulty as user improves

### **2. Dynamic Mock Exams:**
- **Filter, don't generate:** Only use real CBR questions
- **Focus on weaknesses:** Prioritize topics user struggles with
- **Maintain format:** Always 15 regular + 10 image
- **Preserve authenticity:** All questions are real exam questions

### **3. Data Flow:**
```
User Performance Data
    ‚Üì
AI Analysis (aiCoach.ts)
    ‚Üì
Difficulty Calculation / Weak Area Identification
    ‚Üì
Question Selection (Filter from real questions)
    ‚Üì
Personalized Test / Exam
    ‚Üì
Track Results
    ‚Üì
Update Performance Data
    ‚Üì
(Repeat)
```

---

## ‚úÖ VALIDATION LOGIC

### **Before Generating Exam:**
1. ‚úÖ Check: All questions from realExamQuestions or mockExamImageQuestions?
2. ‚úÖ Check: Format is 15 regular + 10 image?
3. ‚úÖ Check: All questions have isRealExam: true?
4. ‚úÖ Check: Questions match weak areas?
5. ‚úÖ Check: Difficulty distribution appropriate?

### **After User Takes Exam:**
1. ‚úÖ Track: Which questions answered correctly?
2. ‚úÖ Track: Time spent per question?
3. ‚úÖ Update: Performance per topic
4. ‚úÖ Recalculate: Weak areas for next exam
5. ‚úÖ Adjust: Difficulty level for future tests

---

## üéì WHY THIS WORKS

### **Adaptive Learning:**
- Users see questions at their level ‚Üí Less frustration
- Difficulty increases gradually ‚Üí Natural progression
- Real-time adjustment ‚Üí Immediate feedback
- Better learning efficiency ‚Üí Faster improvement

### **Dynamic Mock Exams:**
- Focus on weak areas ‚Üí Targeted practice
- Real CBR questions ‚Üí Authentic exam experience
- Personalized ‚Üí More relevant practice
- Better exam readiness ‚Üí Higher pass rates

